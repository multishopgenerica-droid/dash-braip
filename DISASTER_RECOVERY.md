# ğŸ†˜ DISASTER_RECOVERY.md - Plano de RecuperaÃ§Ã£o de Desastres

> **Objetivo:** Restaurar operaÃ§Ãµes o mais rÃ¡pido possÃ­vel apÃ³s qualquer incidente
> **Mantra:** "Planejar para o pior, esperar o melhor"

---

## ğŸ“Š CLASSIFICAÃ‡ÃƒO DE DESASTRES

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   NÃVEIS DE SEVERIDADE                                                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘   ğŸ”´ SEV1 - CRÃTICO                                                           â•‘
â•‘   â€¢ Sistema completamente fora do ar                                          â•‘
â•‘   â€¢ Perda de dados em andamento                                               â•‘
â•‘   â€¢ Impacto financeiro imediato                                               â•‘
â•‘   â€¢ RTO: 15 minutos | RPO: 1 hora                                             â•‘
â•‘                                                                               â•‘
â•‘   ğŸŸ  SEV2 - ALTO                                                              â•‘
â•‘   â€¢ Funcionalidade crÃ­tica indisponÃ­vel                                       â•‘
â•‘   â€¢ Performance severamente degradada                                         â•‘
â•‘   â€¢ Afeta maioria dos usuÃ¡rios                                                â•‘
â•‘   â€¢ RTO: 30 minutos | RPO: 4 horas                                            â•‘
â•‘                                                                               â•‘
â•‘   ğŸŸ¡ SEV3 - MÃ‰DIO                                                             â•‘
â•‘   â€¢ Funcionalidade secundÃ¡ria afetada                                         â•‘
â•‘   â€¢ Workaround disponÃ­vel                                                     â•‘
â•‘   â€¢ Afeta alguns usuÃ¡rios                                                     â•‘
â•‘   â€¢ RTO: 2 horas | RPO: 24 horas                                              â•‘
â•‘                                                                               â•‘
â•‘   ğŸŸ¢ SEV4 - BAIXO                                                             â•‘
â•‘   â€¢ Problema cosmÃ©tico/menor                                                  â•‘
â•‘   â€¢ NÃ£o afeta operaÃ§Ãµes                                                       â•‘
â•‘   â€¢ RTO: 24 horas | RPO: 48 horas                                             â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RTO = Recovery Time Objective (tempo mÃ¡ximo para restaurar)
RPO = Recovery Point Objective (perda mÃ¡xima aceitÃ¡vel de dados)
```

---

## ğŸš¨ PROCEDIMENTO DE EMERGÃŠNCIA GERAL

### Passo 1: NÃ£o Entre em PÃ¢nico!

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   ğŸ§˜ MANTENHA A CALMA                                                         â•‘
â•‘                                                                               â•‘
â•‘   â€¢ Respire fundo                                                             â•‘
â•‘   â€¢ NÃ£o tome decisÃµes precipitadas                                            â•‘
â•‘   â€¢ Siga este documento passo a passo                                         â•‘
â•‘   â€¢ Documente TUDO que fizer                                                  â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Passo 2: Identificar e Classificar

1. O que estÃ¡ acontecendo exatamente?
2. Quando comeÃ§ou?
3. Qual Ã© o impacto?
4. Quantos usuÃ¡rios afetados?
5. Qual severidade (SEV1-4)?

### Passo 3: Comunicar

```
COMUNICAÃ‡ÃƒO IMEDIATA:

1. Slack/Discord do time: "ğŸš¨ INCIDENTE SEV[X]: [descriÃ§Ã£o breve]"
2. Stakeholders (se SEV1/2)
3. AtualizaÃ§Ã£o a cada 15 minutos
```

### Passo 4: Mitigar

- SEV1/2: Primeiro ESTABILIZAR, depois investigar causa raiz
- SEV3/4: Pode investigar enquanto monitora

### Passo 5: Resolver

- Aplicar fix
- Verificar resoluÃ§Ã£o
- Monitorar por 30 minutos

### Passo 6: Documentar

- Preencher INCIDENT_LOG.md
- Agendar post-mortem (SEV1/2)

---

## ğŸ”¥ CENÃRIOS DE DESASTRE E RESPOSTAS

### CenÃ¡rio 1: Servidor/AplicaÃ§Ã£o Caiu

**Sintomas:**
- Site/API nÃ£o responde
- Containers reiniciando

**DiagnÃ³stico:**
```bash
# 1. Verificar status dos containers
docker ps -a

# 2. Ver logs
docker logs <container> --tail 200

# 3. Verificar recursos
docker stats
free -m
df -h
```

**ResoluÃ§Ã£o:**

```bash
# OpÃ§Ã£o A: Reiniciar serviÃ§o
docker compose restart app

# OpÃ§Ã£o B: Rebuild completo
docker compose down
docker compose up -d --build

# OpÃ§Ã£o C: Rollback para versÃ£o anterior
git checkout HEAD~1
docker compose up -d --build

# OpÃ§Ã£o D: Usar DEPLOY_SAFE.sh
./DEPLOY_SAFE.sh --rollback
```

---

### CenÃ¡rio 2: Banco de Dados Corrompido/InacessÃ­vel

**Sintomas:**
- Erros de conexÃ£o com banco
- Queries falhando
- Dados inconsistentes

**DiagnÃ³stico:**
```bash
# 1. Verificar container do banco
docker logs postgres --tail 100

# 2. Testar conexÃ£o
psql -h localhost -U user -d dbname -c "SELECT 1"

# 3. Verificar espaÃ§o em disco
docker exec postgres df -h
```

**ResoluÃ§Ã£o:**

```bash
# Se container travou:
docker restart postgres
# Aguardar 30s

# Se banco corrompido, restaurar backup:
# 1. Parar aplicaÃ§Ã£o
docker compose stop app

# 2. Listar backups disponÃ­veis
ls -la backups/

# 3. Restaurar Ãºltimo backup vÃ¡lido
gunzip -c backups/db_TIMESTAMP.sql.gz | docker exec -i postgres psql -U user -d dbname

# 4. Reiniciar aplicaÃ§Ã£o
docker compose up -d app
```

---

### CenÃ¡rio 3: Disco Cheio

**Sintomas:**
- Erros de "No space left on device"
- Banco nÃ£o aceita writes
- Logs param de ser gravados

**ResoluÃ§Ã£o Imediata:**
```bash
# 1. Verificar uso
df -h

# 2. Identificar maiores consumidores
du -sh /* 2>/dev/null | sort -hr | head -20

# 3. Limpar Docker (CUIDADO!)
docker system prune -f
docker volume prune -f

# 4. Limpar logs antigos
find /var/log -name "*.log" -mtime +7 -delete
truncate -s 0 /var/log/syslog

# 5. Limpar backups antigos (manter Ãºltimos 5)
ls -t backups/*.sql.gz | tail -n +6 | xargs rm -f
```

---

### CenÃ¡rio 4: Ataque DDoS / Sobrecarga

**Sintomas:**
- Resposta muito lenta
- Timeouts frequentes
- CPU/memÃ³ria no limite

**ResoluÃ§Ã£o Imediata:**
```bash
# 1. Ativar modo manutenÃ§Ã£o (se disponÃ­vel)
# Ex: criar arquivo /maintenance.html

# 2. Bloquear IPs suspeitos (se identificados)
# No firewall ou Cloudflare

# 3. Escalar horizontalmente (se possÃ­vel)
docker service scale app=5

# 4. Ativar rate limiting mais agressivo
```

---

### CenÃ¡rio 5: Credenciais Vazadas

**Sintomas:**
- Alerta de seguranÃ§a
- Commit com secrets
- Acesso nÃ£o autorizado

**AÃ‡ÃƒO IMEDIATA:**
```bash
# 1. REVOGAR CREDENCIAIS IMEDIATAMENTE
# - API keys
# - Senhas de banco
# - Tokens de acesso
# (Fazer isso no painel de cada serviÃ§o)

# 2. Gerar novas credenciais

# 3. Atualizar .env em todos os ambientes

# 4. Fazer deploy com novas credenciais

# 5. Auditar acessos
# - Verificar logs de acesso
# - Identificar uso indevido

# 6. Se cÃ³digo foi commitado com secrets:
# (NÃ£o adianta apenas remover - histÃ³rico Git guarda)
git filter-branch --force --index-filter \
  "git rm --cached --ignore-unmatch arquivo-com-secrets" \
  --prune-empty --tag-name-filter cat -- --all
git push --force --all
```

---

### CenÃ¡rio 6: Dados Deletados Acidentalmente

**Sintomas:**
- Registros sumiram
- Tabela/collection vazia
- UsuÃ¡rio reportou perda de dados

**ResoluÃ§Ã£o:**
```bash
# 1. PARE! NÃ£o faÃ§a mais nada no banco

# 2. Identificar momento da deleÃ§Ã£o
# - Logs da aplicaÃ§Ã£o
# - Logs do banco

# 3. Avaliar opÃ§Ãµes:

# OpÃ§Ã£o A: Point-in-time recovery (se configurado)
# Usar WAL do PostgreSQL para restaurar atÃ© momento especÃ­fico

# OpÃ§Ã£o B: Restaurar backup completo
docker compose stop app
gunzip -c backups/db_PRE_DELEÃ‡ÃƒO.sql.gz | psql ...
docker compose up -d app

# OpÃ§Ã£o C: Restaurar apenas tabela afetada
# Extrair tabela do backup e importar
```

---

## ğŸ’¾ ESTRATÃ‰GIA DE BACKUP

### FrequÃªncia Recomendada

| Tipo | FrequÃªncia | RetenÃ§Ã£o | Destino |
|------|------------|----------|---------|
| Full DB | DiÃ¡rio 3h | 30 dias | S3/Remoto |
| Incremental | 6 em 6h | 7 dias | Local + S3 |
| Arquivos | DiÃ¡rio | 30 dias | S3 |
| Configs | A cada mudanÃ§a | Indefinido | Git |

### Script de Backup AutomÃ¡tico

```bash
#!/bin/bash
# backup.sh

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR=/backups
S3_BUCKET=s3://meu-bucket/backups

# Backup do banco
docker exec postgres pg_dump -U user dbname | gzip > $BACKUP_DIR/db_$TIMESTAMP.sql.gz

# Upload para S3
aws s3 cp $BACKUP_DIR/db_$TIMESTAMP.sql.gz $S3_BUCKET/

# Limpar backups locais antigos (manter 7 dias)
find $BACKUP_DIR -name "*.sql.gz" -mtime +7 -delete

echo "Backup concluÃ­do: db_$TIMESTAMP.sql.gz"
```

### Testar RestauraÃ§Ã£o

```
âš ï¸ IMPORTANTE: Testar restauraÃ§Ã£o MENSALMENTE!

1. Pegar backup aleatÃ³rio
2. Restaurar em ambiente isolado
3. Verificar integridade dos dados
4. Documentar resultado
```

---

## ğŸ“ CONTATOS DE EMERGÃŠNCIA

| FunÃ§Ã£o | Nome | Telefone | Quando acionar |
|--------|------|----------|----------------|
| Dev Lead | [Nome] | [Tel] | SEV1/2 |
| DevOps | [Nome] | [Tel] | Infra down |
| DBA | [Nome] | [Tel] | Banco corrompido |
| SeguranÃ§a | [Nome] | [Tel] | Vazamento/Ataque |
| Gerente | [Nome] | [Tel] | SEV1 prolongado |

### ServiÃ§os Externos

| ServiÃ§o | Suporte | NÃ­vel |
|---------|---------|-------|
| AWS | aws.amazon.com/support | Business |
| Cloudflare | support.cloudflare.com | Pro |
| Banco (ex: Neon) | [link suporte] | [nÃ­vel] |

---

## ğŸ“‹ CHECKLIST PÃ“S-INCIDENTE

### Imediato (atÃ© 24h)

- [ ] Sistema estabilizado
- [ ] ComunicaÃ§Ã£o enviada para afetados
- [ ] INCIDENT_LOG.md atualizado
- [ ] Monitoramento reforÃ§ado

### Curto prazo (atÃ© 1 semana)

- [ ] Post-mortem realizado (SEV1/2)
- [ ] Causa raiz identificada
- [ ] AÃ§Ãµes preventivas definidas
- [ ] Timeline documentada

### MÃ©dio prazo (atÃ© 1 mÃªs)

- [ ] AÃ§Ãµes preventivas implementadas
- [ ] DocumentaÃ§Ã£o atualizada
- [ ] Treinamento do time (se necessÃ¡rio)
- [ ] RevisÃ£o de processos

---

## ğŸ”„ RUNBOOKS

### Runbook: Reiniciar ProduÃ§Ã£o

```bash
# 1. Comunicar
echo "ğŸ”„ Iniciando restart de produÃ§Ã£o"

# 2. Colocar em manutenÃ§Ã£o (se disponÃ­vel)

# 3. Restart gradual
docker compose restart redis
sleep 10
docker compose restart postgres
sleep 30
docker compose restart app
sleep 10

# 4. Verificar saÃºde
curl http://localhost:3000/health

# 5. Comunicar conclusÃ£o
echo "âœ… Restart concluÃ­do"
```

### Runbook: Failover para Backup

```bash
# 1. Identificar que primÃ¡rio estÃ¡ down

# 2. Promover rÃ©plica (se houver)
# EspecÃ­fico por banco (consultar docs)

# 3. Atualizar DNS/Load Balancer
# Apontar para novo primÃ¡rio

# 4. Notificar time

# 5. Planejar reconstruÃ§Ã£o do primÃ¡rio
```

---

## ğŸ“Š MÃ‰TRICAS PARA MONITORAR

### Indicadores de Problema

| MÃ©trica | Normal | Alerta | CrÃ­tico |
|---------|--------|--------|---------|
| Error rate | < 0.1% | > 1% | > 5% |
| LatÃªncia p99 | < 500ms | > 1s | > 5s |
| CPU | < 60% | > 80% | > 95% |
| MemÃ³ria | < 70% | > 85% | > 95% |
| Disco | < 70% | > 85% | > 95% |
| ConexÃµes DB | < 70% | > 85% | > 95% |

### Alertas Configurados

```yaml
# Exemplo: Prometheus/Alertmanager
- alert: HighErrorRate
  expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
  for: 2m
  labels:
    severity: critical
  annotations:
    summary: "Alto Ã­ndice de erros"
```

---

## ğŸ” SEGURANÃ‡A EM DESASTRES

### PrincÃ­pios

1. **Menor privilÃ©gio:** Mesmo em emergÃªncia, usar credenciais mÃ­nimas necessÃ¡rias
2. **Audit trail:** Logar TUDO que fizer durante incidente
3. **Dois pares de olhos:** AÃ§Ãµes crÃ­ticas com segundo aprovador
4. **ComunicaÃ§Ã£o segura:** NÃ£o enviar credenciais por canais inseguros

### Acesso de EmergÃªncia

```
âš ï¸ Credenciais de emergÃªncia devem:
- Ser guardadas em local seguro (cofre fÃ­sico ou digital)
- Ser atualizadas apÃ³s cada uso
- Ter uso logado automaticamente
- Requerer justificativa
```

---

*Em caso de desastre: CALMA, PROCESSO, DOCUMENTAÃ‡ÃƒO! ğŸ†˜*
